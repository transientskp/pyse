import os
import unittest
import numpy as np
import warnings
from pathlib import Path
import pytest
from scipy.signal import convolve
from scipy.stats import ttest_1samp
import pandas as pd

from astropy.wcs import WCS
from astropy.coordinates import SkyCoord
import astropy.units as u

from sourcefinder.accessors import sourcefinder_image_from_accessor, writefits
from sourcefinder.accessors.fitsimage import FitsImage
from .conftest import DATAPATH
from sourcefinder.testutil.decorators import requires_data, duration
import sourcefinder.accessors
from sourcefinder import image
from sourcefinder.config import Conf, ImgConf, ExportSettings
from sourcefinder.utility.sourceparams import SourceParams
from sourcefinder.deconv import covariance_matrix
from sourcefinder.testutil.convolve import (
    gaussian_from_Sigma_matrix,
    convolve_gaussians,
    params_from_sigma,
)

# Thresholds and other numbers for unit tests in this module.
MAX_BIAS = 5.0
# This is the maximum factor by which the standard deviation of the
# normalized residuals may deviate from 1.0.
STD_MAX_BIAS_FACTOR = 2.0

# The numbers below only apply to the tests in the SourceParameters class.
NUMBER_INSERTED = 3969
TRUE_PEAK_BRIGHTNESS = 1063.67945065
TRUE_DECONV_SMAJ = 2.0 * 5.5956 / 2.0
TRUE_DECONV_SMIN = 0.5 * 4.6794 / 2.0
TRUE_DECONV_BPA = -0.5 * (-49.8)
# These are measured from the file CORRELATED_NOISE.FITS.
# BG_MEAN = numpy.mean(sourcefinder_image_from_accessor(FitsFile("CORRELATED_NOISE.FITS")).data)
BG_MEAN = -0.0072340798975137829
# BG_STD = numpy.std(sourcefinder_image_from_accessor(FitsFile("CORRELATED_NOISE.FITS")).data)
BG_STD = 5.3480336747739079


class SourceParameters(unittest.TestCase):
    """
    This is a set of unit tests in the presence of correlated noise.
    That noise was generated by convolving uncorrelated noise with the
    dirty beam of a certain VLA observation. 3969 identical extended
    sources on a regular 63*63 grid were convolved with the clean beam.
    The accuracy of the deconvolution algorithm, i.e., the deconvolution
    of the fitted parameters from the clean beam. is tested.
    It also tests the accuracy of the peak brightness measurements.
    Bias up to 5 sigma is allowed.
    Note that oversampling of the synthesized beam will likely reduce bias,
    i.e. pixellation effects will become apparent.
    """

    def setUp(self):
        fitsfile = sourcefinder.accessors.open(
            os.path.join(DATAPATH, "deconvolved.fits")
        )
        img = image.ImageData(
            fitsfile.data,
            fitsfile.beam,
            fitsfile.wcs,
            conf=Conf(
                ImgConf(detection_thr=10.0, analysis_thr=6.0), ExportSettings()
            ),
        )

        # This is quite subtle. We bypass any possible flaws in the
        # kappa, sigma clipping algorithm by supplying a background
        # level and noise map. In this way we make sure that any
        # possible biases in the measured source parameters cannot
        # come from biases in the background level. The peak brightnesses,
        # in particular, can be biased low if the background levels
        # are biased high. The background and noise levels supplied
        # here are the true values.

        extraction_results = img.extract(
            noisemap=np.ma.array(BG_STD * np.ones((2048, 2048))),
            bgmap=np.ma.array(BG_MEAN * np.ones((2048, 2048))),
        )
        self.number_sources = len(extraction_results)

        peak_brightnesses = []
        deconv_smajaxes = []
        deconv_sminaxes = []
        deconv_bpas = []

        for source in extraction_results:
            peak_brightnesses.append([source.peak.value, source.peak.error])
            deconv_smajaxes.append(
                [source.smaj_dc.value, source.smaj_dc.error]
            )
            deconv_sminaxes.append(
                [source.smin_dc.value, source.smin_dc.error]
            )
            deconv_bpas.append([source.theta_dc.value, source.theta_dc.error])

        self.peak_brightnesses = np.array(peak_brightnesses)
        self.deconv_smajaxes = np.array(deconv_smajaxes)
        self.deconv_sminaxes = np.array(deconv_sminaxes)
        self.deconv_bpas = np.array(deconv_bpas)

    @duration(100)
    @requires_data(os.path.join(DATAPATH, "deconvolved.fits"))
    def testAllParameters(self):
        # Test all deconvolved
        self.assertEqual(
            np.where(np.isnan(self.deconv_smajaxes), 1, 0).sum(), 0
        )
        self.assertEqual(
            np.where(np.isnan(self.deconv_sminaxes), 1, 0).sum(), 0
        )
        self.assertEqual(np.where(np.isnan(self.deconv_bpas), 1, 0).sum(), 0)

        # Test number of sources
        self.assertEqual(self.number_sources, NUMBER_INSERTED)

        # Test peak brightnesses
        peak_weights = 1.0 / self.peak_brightnesses[:, 1] ** 2
        sum_peak_weights = np.sum(peak_weights)
        av_peak = np.sum(
            self.peak_brightnesses[:, 0] * peak_weights / sum_peak_weights
        )
        av_peak_err = 1 / np.sqrt(sum_peak_weights)
        signif_dev_peak = (TRUE_PEAK_BRIGHTNESS - av_peak) / av_peak_err
        self.assertTrue(np.abs(signif_dev_peak) < MAX_BIAS)

        # Test major axes
        smaj_weights = 1.0 / self.deconv_smajaxes[:, 1] ** 2
        sum_smaj_weights = np.sum(smaj_weights)
        av_smaj = np.sum(
            self.deconv_smajaxes[:, 0] * smaj_weights / sum_smaj_weights
        )
        av_smaj_err = 1 / np.sqrt(sum_smaj_weights)
        signif_dev_smaj = (TRUE_DECONV_SMAJ - av_smaj) / av_smaj_err
        self.assertTrue(np.abs(signif_dev_smaj) < MAX_BIAS)

        # Test minor axes
        smin_weights = 1.0 / self.deconv_sminaxes[:, 1] ** 2
        sum_smin_weights = np.sum(smin_weights)
        av_smin = np.sum(
            self.deconv_sminaxes[:, 0] * smin_weights / sum_smin_weights
        )
        av_smin_err = 1 / np.sqrt(sum_smin_weights)
        signif_dev_smin = (TRUE_DECONV_SMIN - av_smin) / av_smin_err
        self.assertTrue(np.abs(signif_dev_smin) < MAX_BIAS)

        # Test position angles
        bpa_weights = 1.0 / self.deconv_bpas[:, 1] ** 2
        sum_bpa_weights = np.sum(bpa_weights)
        av_bpa = np.sum(self.deconv_bpas[:, 0] * bpa_weights / sum_bpa_weights)
        av_bpa_err = 1 / np.sqrt(sum_bpa_weights)
        signif_dev_bpa = (TRUE_DECONV_BPA - av_bpa) / av_bpa_err
        self.assertTrue(np.abs(signif_dev_bpa) < MAX_BIAS)


@pytest.fixture
def generate_artificial_image_fixture(tmp_path):
    return generate_artificial_image(tmp_path)


def generate_artificial_image(tmp_path):
    """Generate FITS image with either resolved or unresolved sources and save
    ground truth."""

    @requires_data(os.path.join(DATAPATH, "Dirty_beam", "psf.fits"))
    def _generate(
        psf_fits_path: Path = os.path.join(DATAPATH, "Dirty_beam", "psf.fits"),
        output_fits_path: Path = tmp_path / "image_unresolved.fits",
        output_truth_path: Path = tmp_path / "truth_unresolved.h5",
        output_size=4096,
        peak_brightness=50.0,
        num_sources=167_281,
        rng=np.random.default_rng(13302),
        resolved_shape=None,
    ):

        psf_fits = FitsImage(psf_fits_path)
        psf_im = sourcefinder_image_from_accessor(psf_fits)
        psf_imdata = psf_im.data.data

        # Convolve white noise with PSF
        white_noise = rng.normal(0, 1, (output_size, output_size))
        psf_kernel = psf_imdata / np.sum(psf_imdata)
        corr_noise = convolve(white_noise, psf_kernel, mode="same")

        # Normalize to mean 0, std 1 Jy/beam
        corr_noise = (corr_noise - np.mean(corr_noise)) / np.std(corr_noise)

        coords = []

        # Round num_sources to the nearest squared integer.
        ns_sqrt = round(np.sqrt(num_sources))
        ns_sqi = ns_sqrt**2

        source_spacing = round(output_size / (ns_sqrt + 1))

        size = source_spacing
        # Best to have a centrosymmetric grid, which requires size to be odd.
        if size % 2 == 0:
            size -= 1

        # Creates indices starting at 0, ending at size - 1.
        xx, yy = np.indices((size, size), dtype=float)

        center = (size - 1) // 2
        # This should ensure that both x.min() and y.min() are integers and equal.
        xx -= center
        yy -= center

        assert xx.mean() == 0
        assert yy.mean() == 0

        xx_min = xx.min()
        yy_min = yy.min()

        # Space around the center of the Gaussian.
        space_ar = (source_spacing - 1) // 2
        sigma_to_ax = np.sqrt(2 * np.log(2))
        # Convert to semi-major axes to stds, since the covariance matrix
        # function expects stds.
        sigma_psf_im_beam = (
            psf_im.beam[0] / sigma_to_ax,
            psf_im.beam[1] / sigma_to_ax,
            psf_im.beam[2],
        )
        Sigma_psf_inner_lobe = covariance_matrix(*sigma_psf_im_beam)

        if resolved_shape is not None:
            # Convert to semi-major axes to stds, since the covariance matrix
            # function expects stds.
            sigma_resolved_shape = (
                resolved_shape[0] / sigma_to_ax,
                resolved_shape[1] / sigma_to_ax,
                resolved_shape[2],
            )
            Sigma_resolved_source = covariance_matrix(*sigma_resolved_shape)

            # Analytic convolution of two Gaussians.
            # The returned peak is unimportant now, since we will scale it
            # later. Same for the offset, since we will be adding a random
            # offset.
            _, _, Sigma_H = convolve_gaussians(
                1,
                np.array([0.0, 0.0]),
                Sigma_psf_inner_lobe,
                1,
                np.array([0.0, 0.0]),
                Sigma_resolved_source,
            )
            sigma_maj_convolved, sigma_min_convolved, theta_convolved = (
                params_from_sigma(Sigma_H)
            )

        for x in np.linspace(
            space_ar + 1,
            output_size - space_ar - 1,
            num=ns_sqrt,
            endpoint=True,
        ):
            for y in np.linspace(
                space_ar + 1,
                output_size - space_ar - 1,
                num=ns_sqrt,
                endpoint=True,
            ):

                offset_x = rng.uniform(-0.5, 0.5)
                offset_y = rng.uniform(-0.5, 0.5)

                offset_arr = np.array([offset_x, offset_y])

                roundx = round(x)
                roundy = round(y)

                pos_x = roundx + offset_x
                pos_y = roundy + offset_y

                if resolved_shape is None:
                    source_to_be_inserted = gaussian_from_Sigma_matrix(
                        xx,
                        yy,
                        peak_brightness,
                        offset_arr,
                        Sigma_psf_inner_lobe,
                    )
                else:
                    source_to_be_inserted = gaussian_from_Sigma_matrix(
                        xx, yy, peak_brightness, offset_arr, Sigma_H
                    )

                subimage_indices = (
                    slice(roundx - space_ar, roundx + space_ar + 1),
                    slice(roundy - space_ar, roundy + space_ar + 1),
                )

                # Only add ground-truth values and insert sources if all tests
                # pass.
                all_tests_pass = 1

                try:
                    lowest_x_subimage = subimage_indices[0].start - roundx
                    assert lowest_x_subimage == xx_min
                except AssertionError:
                    warnings.warn(
                        (
                            f"Lower x-bounds not aligned, {lowest_x_subimage = } ,"
                            f"{xx_min = }"
                        )
                    )
                    all_tests_pass = 0

                try:
                    lowest_y_subimage = subimage_indices[1].start - roundy
                    assert lowest_y_subimage == yy_min
                except AssertionError:
                    warnings.warn(
                        (
                            f"Lower y-bounds not aligned, {lowest_y_subimage = } ,"
                            f"{yy_min = }"
                        )
                    )
                    all_tests_pass = 0

                try:
                    assert (
                        subimage_indices[0].stop - subimage_indices[0].start
                        == source_to_be_inserted.shape[0]
                    )
                    try:
                        assert (
                            subimage_indices[1].stop
                            - subimage_indices[1].start
                            == source_to_be_inserted.shape[1]
                        )
                    except AssertionError:
                        subimage_width_y = (
                            subimage_indices[1].stop
                            - subimage_indices[1].start
                        )
                        source_inserted_width_y = source_to_be_inserted.shape[
                            1
                        ]
                        warnings.warn(
                            (
                                f"{subimage_width_y = }, "
                                f"{source_inserted_width_y = }: "
                                "assertion failed."
                            )
                        )
                        all_tests_pass = 0

                except AssertionError:
                    subimage_width_x = (
                        subimage_indices[0].stop - subimage_indices[0].start
                    )
                    source_inserted_width_x = source_to_be_inserted.shape[0]
                    warnings.warn(
                        (
                            f"{subimage_width_x = }, "
                            f"{source_inserted_width_x = }: "
                            "assertion failed."
                        )
                    )
                    all_tests_pass = 0

                if all_tests_pass:
                    # Insert the source in the image, i.e. add it to the
                    # correlated noise.
                    corr_noise[subimage_indices] += source_to_be_inserted
                    coords.append((pos_x, pos_y))

        # Construct output header
        out_header = psf_fits.header.copy()
        out_header["NAXIS1"] = output_size
        out_header["NAXIS2"] = output_size
        out_header["CRPIX1"] = output_size // 2
        out_header["CRPIX2"] = output_size // 2

        wcs_out = WCS(out_header)

        print()
        print(f"Number of sources intended to be inserted = {num_sources}")
        print(
            (
                "Number of sources intended to be inserted matching a square "
                f"regular grid = {ns_sqi}"
            )
        )
        coords_px = np.array(coords)
        print(f"Number of sources actually inserted = {len(coords)}")
        coords_full = np.zeros((ns_sqi, 4))
        coords_full[:, 0] = coords_px[:, 0]  # x
        coords_full[:, 1] = coords_px[:, 1]  # y
        coords_full[:, 2] = 0  # freq index
        coords_full[:, 3] = 0  # stokes index

        coords_world = wcs_out.wcs_pix2world(coords_full, 0)

        truth_dict = {
            SourceParams.PEAK: peak_brightness * np.ones(len(coords_px)),
            SourceParams.X: coords_px[:, 0],
            SourceParams.Y: coords_px[:, 1],
            SourceParams.RA: coords_world[:, 0],
            SourceParams.DEC: coords_world[:, 1],
        }

        if resolved_shape is not None:
            truth_dict[SourceParams.SMAJ_DC] = resolved_shape[0] * np.ones(
                len(coords_px)
            )
            truth_dict[SourceParams.SMIN_DC] = resolved_shape[1] * np.ones(
                len(coords_px)
            )
            truth_dict[SourceParams.THETA_DC] = resolved_shape[2] * np.ones(
                len(coords_px)
            )
            truth_dict[SourceParams.SMAJ] = (
                sigma_maj_convolved * sigma_to_ax * np.ones(len(coords_px))
            )
            truth_dict[SourceParams.SMIN] = (
                sigma_min_convolved * sigma_to_ax * np.ones(len(coords_px))
            )
            truth_dict[SourceParams.THETA] = theta_convolved * np.ones(
                len(coords_px)
            )

        truth_df = pd.DataFrame(truth_dict)

        # Save ground truth to HDF5
        truth_df.to_hdf(output_truth_path, key="truth", mode="w")

        writefits(corr_noise, output_fits_path, header=out_header)

    return _generate


def test_measured_vectorized_forced_beam(
    tmp_path, generate_artificial_image_fixture, min_pvalue=0.01
):
    """
    Compare source parameters from vectorized source measurements with forced
    beam to its corresponding ground truth values. This includes checks for
    biases. The artificial images are regenerated for each test run.
    Consequently, if you run these tests often enough, it will fail at some
    point, depending on the value of `min_pvalue` and `MAX_BIAS`.
    Update: we have set a seed for the random number generator, such that
    this test should now be stable.
    """
    image_path = tmp_path / "image_unresolved.fits"
    truth_path = tmp_path / "truth_unresolved.h5"

    num_sources = 167_281

    # Set maximum allowed bias equal to maximum allowed
    # bias from SourceParameters regression tests times a scaling factor
    # equal to sqrt(num_sources / NUMBER_INSERTED).
    # NUMBER_INSERTED is the number of sources measured in the
    # SourceParameters regression tests.
    MAX_BIAS_SCALED = np.sqrt(num_sources / NUMBER_INSERTED) * MAX_BIAS
    # For vectorized source measurements, we apply "tweaked moments",
    # which results in smaller biases on the brightnesses - compared to
    # Gaussian fits, but still lower than the true values - and larger
    # biases on the elliptical axes than from Gaussian fits, although both
    # methods overestimate the axes.
    MAX_BIAS_BRIGHTNESSES = MAX_BIAS_SCALED / 3.0

    generate_artificial_image_fixture(
        output_fits_path=image_path,
        output_truth_path=truth_path,
        peak_brightness=20.0,
        num_sources=num_sources,
    )

    conf = Conf(
        image=ImgConf(
            detection_thr=12.0,
            analysis_thr=8.0,
            vectorized=True,
            back_size_x=256,
            back_size_y=256,
            force_beam=True,
        ),
        export=ExportSettings(reconvert=False),
    )
    fits_img = FitsImage(image_path)
    img = sourcefinder_image_from_accessor(fits_img, conf=conf)

    source_params_df = img.extract(
        noisemap=np.ma.array(np.ones(img.data.shape)),
        bgmap=np.ma.array(np.zeros(img.data.shape)),
    )
    number_measured_sources = source_params_df.shape[0]

    # Load the ground truth source parameters into a Pandas DataFrame.
    truth_df = pd.read_hdf(truth_path, key="truth")

    assert number_measured_sources == truth_df.shape[0], (
        f"Number of measured sources {number_measured_sources} "
        f"does not match number of ground truth_df sources {truth_df.shape[0]}"
    )

    # Match sources by sky position
    sky_meas = SkyCoord(
        ra=source_params_df[SourceParams.RA].to_numpy() * u.deg,
        dec=source_params_df[SourceParams.DEC].to_numpy() * u.deg,
    )
    sky_truth_df = SkyCoord(
        ra=truth_df[SourceParams.RA].to_numpy() * u.deg,
        dec=truth_df[SourceParams.DEC].to_numpy() * u.deg,
    )

    idx, _, _ = sky_meas.match_to_catalog_sky(sky_truth_df)

    _, counts = np.unique(idx, return_counts=True)
    duplicates = np.sum(counts > 1)
    assert (
        duplicates == 0
    ), f"{duplicates} measured sources were matched to multiple true sources."

    true_x = truth_df[SourceParams.X].to_numpy()[idx]
    true_y = truth_df[SourceParams.Y].to_numpy()[idx]

    measured_x = source_params_df[SourceParams.X].to_numpy()
    measured_y = source_params_df[SourceParams.Y].to_numpy()

    # Compute normalized residuals
    norm_x_resid = (measured_x - true_x) / source_params_df[
        SourceParams.X_ERR
    ].to_numpy()
    norm_y_resid = (measured_y - true_y) / source_params_df[
        SourceParams.Y_ERR
    ].to_numpy()

    # Check that the mean of the position is not biased
    p_x = ttest_1samp(norm_x_resid, popmean=0, nan_policy="raise")[1]
    assert p_x > min_pvalue, f"X position not centred: p={p_x :.3f}"
    p_y = ttest_1samp(norm_y_resid, popmean=0, nan_policy="raise")[1]
    assert p_y > min_pvalue, f"Y position not centred: p={p_y :.3f}"

    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    std_x = np.std(norm_x_resid)
    std_y = np.std(norm_y_resid)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_x < STD_MAX_BIAS_FACTOR, (
        f"X errors not " f"realistic:td={std_x :.3f}"
    )
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_y < STD_MAX_BIAS_FACTOR, (
        f"Y errors not realistic: " f"std={std_y :.3f}"
    )

    # Extract matched true values
    true_ra = truth_df[SourceParams.RA].to_numpy()[idx]
    true_dec = truth_df[SourceParams.DEC].to_numpy()[idx]

    measured_ra = source_params_df[SourceParams.RA].to_numpy()
    measured_dec = source_params_df[SourceParams.DEC].to_numpy()

    measured_ra_err = source_params_df[SourceParams.RA_ERR].to_numpy()
    measured_dec_err = source_params_df[SourceParams.DEC_ERR].to_numpy()

    # Compute normalized residuals
    norm_ra_resid = (measured_ra - true_ra) / measured_ra_err
    norm_dec_resid = (measured_dec - true_dec) / measured_dec_err

    # Check that the mean of the position is not biased
    p_ra = ttest_1samp(norm_ra_resid, popmean=0, nan_policy="raise")[1]
    assert p_ra > min_pvalue, f"Right ascension not centred: p={p_ra :.3f}"
    p_dec = ttest_1samp(norm_dec_resid, popmean=0, nan_policy="raise")[1]
    assert (
        p_dec > min_pvalue
    ), f"DEC residuals deviate too much from normal: p={p_dec :.3f}"

    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    std_ra = np.std(norm_ra_resid)
    std_dec = np.std(norm_dec_resid)

    assert 1.0 / STD_MAX_BIAS_FACTOR < std_ra < STD_MAX_BIAS_FACTOR, (
        f"RA errors not " f"realistic: std={std_ra :.3f}"
    )
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_dec < STD_MAX_BIAS_FACTOR, (
        f"DEC errors not " f"realistic: std" f"={std_dec :.3f}"
    )

    true_peak_brightnesses = truth_df[SourceParams.PEAK].to_numpy()[idx]
    measured_peak_brightnesses = source_params_df[SourceParams.PEAK].to_numpy()
    measured_peak_brightnesses_err = source_params_df[
        SourceParams.PEAK_ERR
    ].to_numpy()

    # Compute normalized residuals
    norm_peak_resid = (
        measured_peak_brightnesses - true_peak_brightnesses
    ) / measured_peak_brightnesses_err

    # Check that the mean of the peak brightnesses is not biased
    t_stat_peak = ttest_1samp(norm_peak_resid, popmean=0, nan_policy="raise")[
        0
    ]
    assert (
        np.abs(t_stat_peak) < MAX_BIAS_BRIGHTNESSES
    ), f"Peak brightnesses severely biased: t_statistic = {t_stat_peak :.3f}"

    std_peak = np.std(norm_peak_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert (
        1.0 / STD_MAX_BIAS_FACTOR < std_peak < STD_MAX_BIAS_FACTOR
    ), f"Uncertainties in peak brightnesses not realistic: std={std_peak :.3f}"


def test_measured_vectorized_free_shape(
    tmp_path, generate_artificial_image_fixture, min_pvalue=0.01
):
    """
    Compare source parameters from vectorized source measurements - without
    forcing the Gaussian shape equal to the clean beam -
    to its corresponding ground truth values. This includes checks for
    biases. The artificial images are regenerated for each test run.
    Consequently, if you run these tests often enough, it will fail at some
    point, depending on the value of `min_pvalue` and `MAX_BIAS`.
    Update: we have set a seed for the random number generator, such that
    this test should now be stable.
    """
    image_path = tmp_path / "image_resolved.fits"
    truth_path = tmp_path / "truth_resolved.h5"

    # Convolved sources have more spread, we do not want them to overlap,
    # therefore we reduce the number of sources compared to the unresolved case.
    num_sources = 40_000

    # Set maximum allowed bias equal to maximum allowed
    # bias from SourceParameters regression tests times a scaling factor
    # equal to sqrt(num_sources / NUMBER_INSERTED).
    # NUMBER_INSERTED is the number of sources measured in the
    # SourceParameters regression tests.
    MAX_BIAS_SCALED = np.sqrt(num_sources / NUMBER_INSERTED) * MAX_BIAS
    # For vectorized source measurements, we apply "tweaked moments",
    # which results in smaller biases on the brightnesses - compared to
    # Gaussian fits, but still lower than the true values - and larger
    # biases on the elliptical axes than from Gaussian fits, although both
    # methods overestimate the axes.
    MAX_BIAS_BRIGHTNESSES = MAX_BIAS_SCALED / 3.0
    MAX_BIAS_AXES = MAX_BIAS_SCALED * 3.0

    # Define an extended source; we are testing whether the deconvolution
    # algorithms can recover this shape.
    smaj_dc = 4.0  # pixels
    smin_dc = 2.0  # pixels
    theta_dc = -37.0  # The default unit in PySE for this quantity is
    # degrees, but for convenience we will pass it on as radians later on.

    generate_artificial_image_fixture(
        output_fits_path=image_path,
        output_truth_path=truth_path,
        peak_brightness=20.0,
        num_sources=num_sources,
        resolved_shape=(smaj_dc, smin_dc, np.deg2rad(theta_dc)),
    )

    conf = Conf(
        image=ImgConf(
            detection_thr=12.0,
            analysis_thr=8.0,
            vectorized=True,
            back_size_x=256,
            back_size_y=256,
            force_beam=False,
        ),
        export=ExportSettings(reconvert=False),
    )
    fits_img = FitsImage(image_path)
    img = sourcefinder_image_from_accessor(fits_img, conf=conf)

    source_params_df = img.extract(
        noisemap=np.ma.array(np.ones(img.data.shape)),
        bgmap=np.ma.array(np.zeros(img.data.shape)),
    )
    number_measured_sources = source_params_df.shape[0]

    # Load the ground truth source parameters into a Pandas DataFrame.
    truth_df = pd.read_hdf(truth_path, key="truth")

    assert number_measured_sources == truth_df.shape[0], (
        f"Number of measured sources {number_measured_sources} "
        f"does not match number of ground truth_df sources {truth_df.shape[0]}"
    )

    # Match sources by sky position.
    sky_meas = SkyCoord(
        ra=source_params_df[SourceParams.RA].to_numpy() * u.deg,
        dec=source_params_df[SourceParams.DEC].to_numpy() * u.deg,
    )
    sky_truth_df = SkyCoord(
        ra=truth_df[SourceParams.RA].to_numpy() * u.deg,
        dec=truth_df[SourceParams.DEC].to_numpy() * u.deg,
    )

    idx, _, _ = sky_meas.match_to_catalog_sky(sky_truth_df)

    _, counts = np.unique(idx, return_counts=True)
    duplicates = np.sum(counts > 1)
    assert (
        duplicates == 0
    ), f"{duplicates} measured sources were matched to multiple true sources."

    true_x = truth_df[SourceParams.X].to_numpy()[idx]
    true_y = truth_df[SourceParams.Y].to_numpy()[idx]

    measured_x = source_params_df[SourceParams.X].to_numpy()
    measured_y = source_params_df[SourceParams.Y].to_numpy()

    # Compute normalized residuals.
    norm_x_resid = (measured_x - true_x) / source_params_df[
        SourceParams.X_ERR
    ].to_numpy()
    norm_y_resid = (measured_y - true_y) / source_params_df[
        SourceParams.Y_ERR
    ].to_numpy()

    # Check that the measurements of the positions are not biased.
    p_x = ttest_1samp(norm_x_resid, popmean=0, nan_policy="raise")[1]
    assert p_x > min_pvalue, f"X position not centred: p={p_x :.3f}"
    p_y = ttest_1samp(norm_y_resid, popmean=0, nan_policy="raise")[1]
    assert p_y > min_pvalue, f"Y position not centred: p={p_y :.3f}"

    # Check standard deviation is ~1 (roughly Gaussian-distributed errors).
    std_x = np.std(norm_x_resid)
    std_y = np.std(norm_y_resid)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_x < STD_MAX_BIAS_FACTOR, (
        f"X errors not " f"realistic:td={std_x :.3f}"
    )
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_y < STD_MAX_BIAS_FACTOR, (
        f"Y errors not realistic: " f"std={std_y :.3f}"
    )

    # Extract matched true values.
    true_ra = truth_df[SourceParams.RA].to_numpy()[idx]
    true_dec = truth_df[SourceParams.DEC].to_numpy()[idx]

    measured_ra = source_params_df[SourceParams.RA].to_numpy()
    measured_dec = source_params_df[SourceParams.DEC].to_numpy()

    measured_ra_err = source_params_df[SourceParams.RA_ERR].to_numpy()
    measured_dec_err = source_params_df[SourceParams.DEC_ERR].to_numpy()

    # Compute normalized residuals.
    norm_ra_resid = (measured_ra - true_ra) / measured_ra_err
    norm_dec_resid = (measured_dec - true_dec) / measured_dec_err

    # Check that the measurements of the positions are not biased.
    p_ra = ttest_1samp(norm_ra_resid, popmean=0, nan_policy="raise")[1]
    assert p_ra > min_pvalue, f"Right ascension not centred: p={p_ra :.3f}"
    p_dec = ttest_1samp(norm_dec_resid, popmean=0, nan_policy="raise")[1]
    assert (
        p_dec > min_pvalue
    ), f"DEC residuals deviate too much from normal: p={p_dec :.3f}"

    # Check standard deviation is ~1 (roughly Gaussian-distributed errors).
    std_ra = np.std(norm_ra_resid)
    std_dec = np.std(norm_dec_resid)

    assert 1.0 / STD_MAX_BIAS_FACTOR < std_ra < STD_MAX_BIAS_FACTOR, (
        f"RA errors not " f"realistic: std={std_ra :.3f}"
    )
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_dec < STD_MAX_BIAS_FACTOR, (
        f"DEC errors not " f"realistic: std" f"={std_dec :.3f}"
    )

    true_peak_brightnesses = truth_df[SourceParams.PEAK].to_numpy()[idx]
    measured_peak_brightnesses = source_params_df[SourceParams.PEAK].to_numpy()
    measured_peak_brightnesses_err = source_params_df[
        SourceParams.PEAK_ERR
    ].to_numpy()

    # Compute normalized residuals
    norm_peak_resid = (
        measured_peak_brightnesses - true_peak_brightnesses
    ) / measured_peak_brightnesses_err

    # Check that the weighted mean of the peak brightnesses is not too biased.
    t_stat_peak = ttest_1samp(norm_peak_resid, popmean=0, nan_policy="raise")[
        0
    ]
    assert (
        np.abs(t_stat_peak) < MAX_BIAS_BRIGHTNESSES
    ), f"Peak brightnesses severely biased: t_statistic = {t_stat_peak :.3f}"

    std_peak = np.std(norm_peak_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_peak < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in peak brightnesses not realistic: std="
        f"{std_peak :.3f}"
    )

    true_smaj = truth_df[SourceParams.SMAJ].to_numpy()[idx]
    measured_smaj = source_params_df[SourceParams.SMAJ].to_numpy()
    measured_smaj_err = source_params_df[SourceParams.SMAJ_ERR].to_numpy()

    # Compute normalized residuals
    norm_smaj_resid = (measured_smaj - true_smaj) / measured_smaj_err

    # Check that the weighted mean of the convolved semi-major axes is not
    # too biased.
    t_stat_smaj = ttest_1samp(norm_smaj_resid, popmean=0, nan_policy="raise")[
        0
    ]
    assert np.abs(t_stat_smaj) < MAX_BIAS_AXES, (
        f"Convolved semi-major axes severely biased: t_statistic ="
        f" {t_stat_smaj :.3f}"
    )

    std_smaj = np.std(norm_smaj_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_smaj < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in convolved semi-major axes not realistic: std"
        f"={std_smaj :.3f}"
    )

    true_smin = truth_df[SourceParams.SMIN].to_numpy()[idx]
    measured_smin = source_params_df[SourceParams.SMIN].to_numpy()
    measured_smin_err = source_params_df[SourceParams.SMIN_ERR].to_numpy()

    # Compute normalized residuals
    norm_smin_resid = (measured_smin - true_smin) / measured_smin_err

    # Check that the weighted mean of the convolved semi-minor axes is not
    # too biased.
    t_stat_smin = ttest_1samp(norm_smin_resid, popmean=0, nan_policy="raise")[
        0
    ]
    assert np.abs(t_stat_smin) < MAX_BIAS_AXES, (
        f"Convolved semi-minor axes severely biased: t_statistic ="
        f" {t_stat_smin :.3f}"
    )

    std_smin = np.std(norm_smin_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_smin < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in convolved semi-major axes not realistic: std"
        f"={std_smin :.3f}"
    )

    true_theta = truth_df[SourceParams.THETA].to_numpy()[idx]
    measured_theta = source_params_df[SourceParams.THETA].to_numpy()
    measured_theta_err = source_params_df[SourceParams.THETA_ERR].to_numpy()

    # Compute normalized residuals.
    norm_theta_resid = (measured_theta - true_theta) / measured_theta_err

    # Check that the weighted mean of the convolved position angles is not
    # biased.
    t_stat_theta = ttest_1samp(
        norm_theta_resid, popmean=0, nan_policy="raise"
    )[0]
    assert np.abs(t_stat_theta) < MAX_BIAS_SCALED, (
        f"Convolved position angles severely biased: t_statistic ="
        f" {t_stat_theta :.3f}"
    )

    std_theta = np.std(norm_theta_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_theta < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in convolved semi-major axes not realistic: std"
        f"={std_theta :.3f}"
    )

    true_smaj_dc = truth_df[SourceParams.SMAJ_DC].to_numpy()[idx]
    measured_smaj_dc = source_params_df[SourceParams.SMAJ_DC].to_numpy()
    measured_smaj_dc_err = source_params_df[
        SourceParams.SMAJ_DC_ERR
    ].to_numpy()

    # Compute normalized residuals
    norm_smaj_dc_resid = (
        measured_smaj_dc - true_smaj_dc
    ) / measured_smaj_dc_err

    # Check that the weighted mean of the deconvolved semi-major axes is not
    # too biased.
    t_stat_smaj_dc = ttest_1samp(
        norm_smaj_dc_resid, popmean=0, nan_policy="raise"
    )[0]
    assert np.abs(t_stat_smaj_dc) < MAX_BIAS_AXES, (
        f"Deconvolved semi-major axes severely biased: t_statistic ="
        f" {t_stat_smaj_dc :.3f}"
    )

    std_smaj_dc = np.std(norm_smaj_dc_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_smaj_dc < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in deconvolved semi-major axes not realistic: std"
        f"={std_smaj_dc :.3f}"
    )

    true_smin_dc = truth_df[SourceParams.SMIN_DC].to_numpy()[idx]
    measured_smin_dc = source_params_df[SourceParams.SMIN_DC].to_numpy()
    measured_smin_dc_err = source_params_df[
        SourceParams.SMIN_DC_ERR
    ].to_numpy()

    # Compute normalized residuals
    norm_smin_dc_resid = (
        measured_smin_dc - true_smin_dc
    ) / measured_smin_dc_err

    # Check that the weighted mean of the deconvolved semi-minor axes is not
    # too biased.
    t_stat_smin_dc = ttest_1samp(
        norm_smin_dc_resid, popmean=0, nan_policy="raise"
    )[0]
    assert np.abs(t_stat_smin_dc) < MAX_BIAS_AXES, (
        f"Deconvolved semi-minor axes severely biased: t_statistic ="
        f" {t_stat_smin_dc :.3f}"
    )

    std_smin_dc = np.std(norm_smin_dc_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_smin_dc < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in deconvolved semi-major axes not realistic: std"
        f"={std_smin_dc :.3f}"
    )

    true_theta_dc = np.rad2deg(truth_df[SourceParams.THETA_DC].to_numpy()[idx])
    measured_theta_dc = source_params_df[SourceParams.THETA_DC].to_numpy()
    measured_theta_dc_err = source_params_df[
        SourceParams.THETA_DC_ERR
    ].to_numpy()

    # Compute normalized residuals.
    norm_theta_dc_resid = (
        measured_theta_dc - true_theta_dc
    ) / measured_theta_dc_err

    # Check that the weighted mean of the deconvolved position angles is not
    # biased.
    t_stat_theta_dc = ttest_1samp(
        norm_theta_dc_resid, popmean=0, nan_policy="raise"
    )[0]
    assert np.abs(t_stat_theta_dc) < MAX_BIAS_SCALED, (
        f"Deconvolved position angles severely biased: t_statistic ="
        f" {t_stat_theta_dc :.3f}"
    )

    std_theta_dc = np.std(norm_theta_dc_resid)
    # Check standard deviation is ~1 (roughly Gaussian-distributed errors)
    assert 1.0 / STD_MAX_BIAS_FACTOR < std_theta_dc < STD_MAX_BIAS_FACTOR, (
        f"Uncertainties in deconvolved semi-major axes not realistic: std"
        f"={std_theta_dc :.3f}"
    )


if __name__ == "__main__":
    unittest.main()
